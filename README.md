# ğŸ“Š Databricks Medallion Architecture â€“ Retail Data Pipeline  
### **Bronze â†’ Silver â†’ Gold | Delta Lake | Unity Catalog | Workflows | GitHub Integration**

## ğŸš€ **Project Overview**  
This project implements a **complete end-to-end data engineering pipeline** using **Databricks Free Edition**, following the **Medallion Architecture** standards widely used in enterprise data platforms.

It processes two datasets:

- `retail_dataset.csv`
- `customer_dataset.json`

Through the layers:

- **Bronze** â†’ Raw ingestion  
- **Silver** â†’ Data cleaning & transformation  
- **Gold** â†’ Business-ready analytical tables  

Additional features:

- âœ” Automations using **Databricks Workflows**  
- âœ” GitHub integration using **Repos (Git folders)**  
- âœ” Scheduled job runs  
- âœ” Email notifications  
- âœ” Delta Lake ACID storage  

---

## ğŸ› **Architecture Diagram**

```
                RAW SOURCE FILES
     (retail_dataset.csv, customer_dataset.json)
                      â”‚
                      â–¼
      ğŸŸ« BRONZE LAYER â€” RAW INGESTION
      - Read from UC Volumes
      - Add ingestion metadata
      - Save as Delta
                      â”‚
                      â–¼
      âšª SILVER LAYER â€” CLEANED DATA
      - Fix date formats
      - Clean price, age, gender, city
      - Remove duplicates
      - Add totalAmount
      - Normalize values
                      â”‚
                      â–¼
      ğŸŸ¡ GOLD LAYER â€” BUSINESS TABLES
      - Customer sales summary
      - Product performance
      - City revenue metrics
      - Loyalty tier analytics
      - Monthly revenue KPIs
                      â”‚
                      â–¼
        ğŸ“Š POWER BI / DATABRICKS SQL DASHBOARDS
```

---

## ğŸ§± **Medallion Layers Explained**

### ğŸŸ« **Bronze Layer â€” Raw Ingestion**  
Location:  
`/Volumes/project4cat/project4db/p4bronze/`

Operations:

- Reads raw CSV/JSON from Unity Catalog Volumes  
- Uses inferSchema for dynamic ingestion  
- Adds metadata:  
  - `ingestion_timestamp`  
  - `source_file`  
- No cleaning â€” direct raw â†’ Delta conversion  

Outputs:  
- `retail_bronze`  
- `customer_bronze`

---

### âšª **Silver Layer â€” Clean & Standardize**  
Location:  
`/Volumes/project4cat/project4db/p4silver/`

#### ğŸ“Œ Customer Data Transformations:
- Standardize gender â†’ *Male / Female / Unknown*
- Clean city: trim, uppercase, null â†’ â€œUNKNOWNâ€
- Age cleanup: negative/invalid â†’ null
- Loyalty tier normalization â†’ uppercase
- Multi-format date parsing for `signup_date`
- Remove duplicates
- Schema enforcement

#### ğŸ“Œ Retail Data Transformations:
- Clean price: remove â€œ$â€, cast to double
- Parse multiple `order_date` formats
- Standardize payment type & order status
- Convert returned â†’ boolean
- Add `totalAmount = price Ã— quantity`
- Remove duplicates

Outputs:  
- `customer_silver`  
- `retail_silver`

---

### ğŸŸ¡ **Gold Layer â€” Analytics & Aggregations**  
Location:  
`/Volumes/project4cat/project4db/p4gold/`

Gold tables include:

### âœ” `sales_gold`  
Enriched fact table (retail + customer join)

### âœ” `customer_sales_summary`
- Total spend  
- Total orders  
- LTV metrics  
- City, gender, loyalty tier breakdowns  

### âœ” `product_sales_summary`
- Units sold  
- Total revenue  
- Order count  

### âœ” `city_sales_summary`
- City-level KPIs  
- Revenue by region  

### âœ” `loyalty_sales_summary`
- Performance of loyalty groups  

### âœ” `monthly_revenue`
- Year-month revenue trend  

---

## âš™ï¸ **Orchestration Using Databricks Workflows**

Pipeline Tasks:

```
bronze_task â†’ silver_task â†’ gold_task
```

Features:

- âœ” Scheduled every 10 minutes (demo mode)  
- âœ” Email alerts on success/failure  
- âœ” Each task runs independently  
- âœ” Automatic end-to-end execution  

---

## ğŸ”„ **GitHub Integration (Databricks Repos)**

This project is fully synced with GitHub using:

- Personal Access Token (PAT)  
- Databricks Repos (Git folder)  
- Commit & Sync from Databricks UI  

You can push notebook changes instantly:

```
Git â†’ Commit & Sync â†’ Push
```

---

## ğŸ“ **Repository Structure**

```
databricks-medallion-retail-pipeline/
â”‚
â”œâ”€â”€ Project4/Notebooks/
â”‚   â”œâ”€â”€ 01_project4_BronzeLayer.ipynb
â”‚   â”œâ”€â”€ 02_project4_SilverLayer.ipynb
â”‚   â”œâ”€â”€ 03_project4_GoldLayer.ipynb
â”‚   â”œâ”€â”€ Project4_Setup.ipynb
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ retail_dataset.csv
â”‚   â”œâ”€â”€ customer_dataset.json
â”‚
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ medallion_architecture.png
â”‚   â”œâ”€â”€ workflow_diagram.png
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ pipeline_overview.md
â”‚   â”œâ”€â”€ gold_tables_description.md
â”‚   â”œâ”€â”€ business_requirements.md
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“ˆ **Example Queries for Dashboards**

### Monthly Revenue
```sql
SELECT month, monthly_revenue 
FROM p4gold.monthly_revenue 
ORDER BY month;
```

### City Revenue
```sql
SELECT city, total_revenue 
FROM p4gold.city_sales_summary 
ORDER BY total_revenue DESC;
```

### Customer LTV
```sql
SELECT customer_id, total_spend, total_orders 
FROM p4gold.customer_sales_summary 
ORDER BY total_spend DESC;
```

---

## ğŸ§ª **How to Reproduce**

### 1ï¸âƒ£ Upload raw data  
Place files into UC Volumes:

```
/Volumes/project4cat/project4db/p4raw/
```

### 2ï¸âƒ£ Run Notebooks (in order):
1. **01 Bronze**  
2. **02 Silver**  
3. **03 Gold**

### 3ï¸âƒ£ Setup Databricks Workflow  
- Add tasks (Bronze â†’ Silver â†’ Gold)  
- Schedule (every 10 min or daily)  
- Add email notifications  

---

## ğŸ¯ **Skills Demonstrated**

- Databricks Unity Catalog  
- Delta Lake (ACID)  
- PySpark transformations  
- GitHub integration  
- Workflow automation  
- Medallion Architecture  
- Data modeling (Fact/Dimension)  
- Dashboard-ready Gold tables  

---

## ğŸ‘¨â€ğŸ’¼ **Author**  
**Tejas Gawali**  
Azure Data Engineer | Databricks | PySpark | Delta Lake | SQL  
GitHub: https://github.com/tejas13gawali
