{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b21db0a-ffbd-4897-98a4-cc739b7d45ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1 Create a Logging Table (Delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b739b59-7f8d-41bd-bf10-2622c86f6640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "log_path = \"/Volumes/project4cat/project4db/p4gold/pipeline_logs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bc75c4a-f8f0-40b8-9f0f-ecba4021d157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame([], \"stage STRING, status STRING, records_in LONG, records_out LONG, run_time STRING, timestamp TIMESTAMP\") \\\n",
    "    .write.format(\"delta\").mode(\"overwrite\").save(log_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06c87726-6cdb-4ca0-bf40-67494d370845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2 Create a Logging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "836ca613-069f-4ad8-a3a0-5e2d5aba259e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This function will log:\n",
    "# Stage name\n",
    "# Status (SUCCESS/FAILED)\n",
    "# Input records\n",
    "# Output records\n",
    "# Run time\n",
    "# Timestamp\n",
    "\n",
    "def log_event(stage, status, records_in=None, records_out=None, run_time=None):\n",
    "    log_df = spark.createDataFrame([\n",
    "        Row(\n",
    "            stage=stage,\n",
    "            status=status,\n",
    "            records_in=records_in,\n",
    "            records_out=records_out,\n",
    "            run_time=run_time,\n",
    "            timestamp=datetime.now()\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    log_df.write.format(\"delta\").mode(\"append\").save(log_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58d5b04d-8ab5-45d1-8f9d-299fc4b5bb24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3 Add Logging to Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8e1c01-2b8e-4de9-97b8-0366114cc8eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "df_raw = spark.read.option(\"header\",\"true\").csv(raw_path)\n",
    "records_in = df_raw.count()\n",
    "\n",
    "df_bronze = df_raw.withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "records_out = df_bronze.count()\n",
    "\n",
    "df_bronze.write.format(\"delta\").mode(\"overwrite\").save(bronze_path)\n",
    "\n",
    "end = datetime.now()\n",
    "run_time = str(end - start)\n",
    "\n",
    "log_event(\"BRONZE_RETAIL\", \"SUCCESS\", records_in, records_out, run_time)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
